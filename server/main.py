from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
from google import genai
from openai import OpenAI
from datetime import datetime
import uvicorn
import os
import time
import unicodedata
import json
import re
from pathlib import Path


app = FastAPI()

# B·∫≠t CORS ƒë·ªÉ Flutter Web g·ªçi ƒë∆∞·ª£c API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Sau n√†y c√≥ th·ªÉ si·∫øt l·∫°i theo domain c·ª• th·ªÉ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class MessageRequest(BaseModel):
    text: str

# ================== C·∫§U H√åNH LOG L·ªäCH S·ª¨ CHAT ==================

LOG_DIR = Path("logs")
LOG_DIR.mkdir(exist_ok=True)  # T·ª± t·∫°o th∆∞ m·ª•c logs n·∫øu ch∆∞a c√≥

LOG_FILE = LOG_DIR / "chat_history.jsonl"  # M·ªói d√≤ng l√† 1 JSON object


def log_chat(
    user_text: str,
    reply: str,
    source: str | None = None,
    faq_id: int | None = None,
    topic: str | None = None,
) -> None:
    """
    Ghi 1 b·∫£n ghi h·ªèi‚Äìƒë√°p v√†o file logs/chat_history.jsonl
    (m·ªói d√≤ng l√† 1 JSON, d·ªÖ ph√¢n t√≠ch sau n√†y).
    """
    try:
        entry = {
            "timestamp": datetime.now().isoformat(timespec="seconds"),
            "user_text": user_text,
            "reply": reply,
            "source": source,  # 'faq' | 'ai' | 'system'
            "faq_id": faq_id,
            "topic": topic,
        }

        with LOG_FILE.open("a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")

    except Exception as e:
        # Kh√¥ng ƒë·ªÉ vi·ªác log l√†m crash server
        print("‚ö† Kh√¥ng ghi ƒë∆∞·ª£c log:", e)



# ====== Load .env & c·∫•u h√¨nh client AI ======
load_dotenv()

# ---- Gemini ----
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
gemini_client = None

if GEMINI_API_KEY:
    try:
        gemini_client = genai.Client(api_key=GEMINI_API_KEY)
        print("‚úÖ Gemini client kh·ªüi t·∫°o th√†nh c√¥ng.")
    except Exception as e:
        print("‚ùå L·ªói khi kh·ªüi t·∫°o Gemini client:", e)
else:
    print("‚ö†Ô∏è Ch∆∞a th·∫•y GEMINI_API_KEY trong m√¥i tr∆∞·ªùng. Fallback AI s·∫Ω kh√¥ng ho·∫°t ƒë·ªông.")

# ---- ChatGPT (OpenAI) ----
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
openai_client = None

if OPENAI_API_KEY:
    try:
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
        print("‚úÖ OpenAI client kh·ªüi t·∫°o th√†nh c√¥ng.")
    except Exception as e:
        print("‚ùå L·ªói khi kh·ªüi t·∫°o OpenAI client:", e)
else:
    print("‚ö†Ô∏è Ch∆∞a th·∫•y OPENAI_API_KEY trong m√¥i tr∆∞·ªùng.")


# ================== C∆† S·ªû TRI TH·ª®C FAQ (15 C√ÇU H·ªéI ‚Äì ƒê√ÅP) ==================
# TODO: Thay n·ªôi dung answer/keywords cho ƒë√∫ng v·ªõi TR∆Ø·ªúNG C·ª¶A B·∫†N

# ================== C∆† S·ªû TRI TH·ª®C FAQ (LOAD T·ª™ faqs.json) ==================

FAQS: list[dict] = []

def load_faqs():
    """
    ƒê·ªçc danh s√°ch FAQ t·ª´ file faqs.json ƒë·∫∑t c√πng th∆∞ m·ª•c v·ªõi main.py
    """
    global FAQS
    faq_path = Path(__file__).parent / "faqs.json"
    try:
        with faq_path.open("r", encoding="utf-8") as f:
            FAQS = json.load(f)
        print(f"‚úÖ ƒê√£ load {len(FAQS)} FAQ t·ª´ {faq_path.name}")
    except FileNotFoundError:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file faqs.json. Vui l√≤ng t·∫°o file n√†y trong th∆∞ m·ª•c server.")
        FAQS = []
    except Exception as e:
        print("‚ùå L·ªói khi ƒë·ªçc faqs.json:", e)
        FAQS = []

# G·ªçi load_faqs khi kh·ªüi ƒë·ªông server
load_faqs()



SYSTEM_PROMPT = """
B·∫°n l√† tr·ª£ l√Ω ·∫£o h·ªó tr·ª£ sinh vi√™n cho m·ªôt tr∆∞·ªùng ƒë·∫°i h·ªçc ·ªü Vi·ªát Nam.
Nhi·ªám v·ª•:
- Gi·∫£i ƒë√°p v·ªÅ tuy·ªÉn sinh, quy ch·∫ø, h·ªçc ph√≠, h·ªçc b·ªïng, th·ªß t·ª•c sinh vi√™n.
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng, ti·∫øng Vi·ªát.
- Khi kh√¥ng ch·∫Øc s·ªë li·ªáu/m·ªëc th·ªùi gian ch√≠nh x√°c, h√£y n√≥i kh√¥ng ch·∫Øc
  v√† khuy√™n sinh vi√™n xem tr√™n website ho·∫∑c li√™n h·ªá ph√≤ng ƒë√†o t·∫°o.
"""

def normalize_vi(text: str) -> str:
    """
    Chu·∫©n h√≥a chu·ªói ti·∫øng Vi·ªát:
    - chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
    - b·ªè kho·∫£ng tr·∫Øng d∆∞ th·ª´a
    - ƒë·ªïi 'ƒë' -> 'd'
    - b·ªè to√†n b·ªô d·∫•u (s·∫Øc, huy·ªÅn, h·ªèi, ng√£, n·∫∑ng, √¢, √™, √¥, ƒÉ, ∆°, ∆∞...)
    """
    text = text.lower().strip()
    text = text.replace("ƒë", "d")
    # t√°ch d·∫•u
    text = unicodedata.normalize("NFD", text)
    # b·ªè k√Ω t·ª± d·∫•u
    text = "".join(ch for ch in text if unicodedata.category(ch) != "Mn")
    return text


def ask_gemini(user_text: str) -> str | None:
    """
    G·ªçi Gemini.
    - Th·ª≠ t·ªëi ƒëa 3 l·∫ßn n·∫øu g·∫∑p l·ªói 503/UNAVAILABLE (model qu√° t·∫£i).
    - Tr·∫£ v·ªÅ chu·ªói n·∫øu OK, None n·∫øu h·∫øt l∆∞·ª£t m√† v·∫´n l·ªói.
    """
    if gemini_client is None:
        return None

    max_retries = 3
    base_delay = 2  # gi√¢y

    for attempt in range(max_retries):
        try:
            response = gemini_client.models.generate_content(
                model="gemini-1.5-flash",
                contents=[
                    SYSTEM_PROMPT,
                    f"Ng∆∞·ªùi d√πng h·ªèi: {user_text}",
                ],
            )

            reply = getattr(response, "text", None)
            if reply:
                return reply
            # g·ªçi ƒë∆∞·ª£c nh∆∞ng r·ªóng -> coi nh∆∞ fail
            return None

        except Exception as e:
            err_str = str(e)
            print(f"L·ªói khi g·ªçi Gemini (l·∫ßn {attempt + 1}):", err_str)

            # N·∫øu l√† l·ªói qu√° t·∫£i 503/UNAVAILABLE v√† c√≤n l∆∞·ª£t th·ª≠
            if ("503" in err_str or "UNAVAILABLE" in err_str) and attempt < max_retries - 1:
                delay = base_delay * (attempt + 1)  # 2s, 4s, ...
                print(f"ƒê·ª£i {delay} gi√¢y r·ªìi th·ª≠ l·∫°i Gemini...")
                time.sleep(delay)
                continue

            # L·ªói kh√°c ho·∫∑c ƒë√£ h·∫øt l∆∞·ª£t retry
            return None

    return None


def ask_chatgpt(user_text: str) -> str | None:
    """
    G·ªçi OpenAI ChatGPT (gpt-4o-mini) l√†m fallback.
    Tr·∫£ v·ªÅ chu·ªói n·∫øu OK, None n·∫øu l·ªói ho·∫∑c ch∆∞a c·∫•u h√¨nh.
    """
    if openai_client is None:
        return None

    try:
        completion = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_text},
            ],
            max_tokens=512,
        )
        reply = completion.choices[0].message.content
        return reply
    except Exception as e:
        print("L·ªói khi g·ªçi ChatGPT:", e)
        return None


def ask_ai_with_fallback(user_text: str) -> str:
    """
    Th·ª≠ Gemini tr∆∞·ªõc, n·∫øu l·ªói/None th√¨ th·ª≠ ChatGPT.
    Cu·ªëi c√πng n·∫øu c·∫£ hai ƒë·ªÅu fail th√¨ tr·∫£ v·ªÅ th√¥ng b√°o chung.
    """
    # 1. Th·ª≠ Gemini
    gemini_reply = ask_gemini(user_text)
    if gemini_reply:
        return gemini_reply

    # 2. Gemini l·ªói / qu√° t·∫£i -> th·ª≠ ChatGPT
    chatgpt_reply = ask_chatgpt(user_text)
    if chatgpt_reply:
        return chatgpt_reply

    # 3. C·∫£ hai ƒë·ªÅu fail
    return (
        "Hi·ªán t·∫°i h·ªá th·ªëng AI ƒëang g·∫∑p s·ª± c·ªë n√™n m√¨nh ch∆∞a tr·∫£ l·ªùi chi ti·∫øt ƒë∆∞·ª£c. "
        "B·∫°n vui l√≤ng th·ª≠ l·∫°i sau ho·∫∑c li√™n h·ªá ph√≤ng ƒë√†o t·∫°o ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£."
    )

# ===============================================================
STOP_KW = {"thong", "tin", "truong", "dai", "hoc", "cao", "dang", "khoa", "nganh"}

def tokenize(norm_text: str) -> list[str]:
    return re.findall(r"[a-z0-9]+", norm_text)

# ================== H√ÄM T√åM C√ÇU TR·∫¢ L·ªúI T·ª™ FAQ ==================

def find_faq_answer(user_text: str) -> dict | None:
    if not user_text:
        return None

    txt_norm = normalize_vi(user_text)
    txt_tokens = set(tokenize(txt_norm))

    # 1) match nguy√™n c√¢u h·ªèi (∆∞u ti√™n tuy·ªát ƒë·ªëi)
    for faq in FAQS:
        questions_norm = [normalize_vi(q) for q in faq.get("questions", [])]
        if txt_norm in questions_norm:
            return {
                "answer": faq.get("answer", ""),
                "topic": faq.get("topic", ""),
                "id": faq.get("id"),
            }

    # 2) match keyword c√≥ ch·∫•m ƒëi·ªÉm (tr√°nh keyword chung chung)
    best = None
    best_score = 0

    for faq in FAQS:
        score = 0
        for kw in faq.get("keywords", []):
            kw_norm = normalize_vi(kw).strip()
            if not kw_norm:
                continue

            kw_tokens = tokenize(kw_norm)
            if not kw_tokens:
                continue

            # b·ªè keyword qu√° chung chung
            if all(t in STOP_KW for t in kw_tokens):
                continue

            # keyword 1 t·ª´ -> match theo token
            if len(kw_tokens) == 1:
                if kw_tokens[0] in txt_tokens:
                    score += 1
            else:
                # keyword nhi·ªÅu t·ª´ -> match theo c·ª•m c√≥ bi√™n t·ª´
                phrase = " ".join(kw_tokens)
                pattern = r"\b" + re.escape(phrase) + r"\b"
                if re.search(pattern, txt_norm):
                    score += 2  # c·ª•m t·ª´ cho ƒëi·ªÉm cao h∆°n

        if score > best_score:
            best_score = score
            best = faq

    # Ng∆∞·ª°ng: ph·∫£i ƒë·ªß ‚Äúch·∫Øc‚Äù m·ªõi coi l√† c√≥ trong FAQ
    if best and best_score >= 2:
        return {
            "answer": best.get("answer", ""),
            "topic": best.get("topic", ""),
            "id": best.get("id"),
        }

    return None





# ================== API CH√çNH /chat ==================

@app.post("/chat")
async def chat_endpoint(request: MessageRequest):
    user_text = request.text.strip()
    print(f"Nh·∫≠n ƒë∆∞·ª£c tin nh·∫Øn: {user_text}")

    if not user_text:
        bot_reply = "B·∫°n h√£y nh·∫≠p c√¢u h·ªèi nh√©, m√¨nh ch∆∞a th·∫•y n·ªôi dung g√¨. üòä"
        # log lu√¥n: c√¢u r·ªóng + system
        log_chat(
            user_text=user_text,
            reply=bot_reply,
            source="system",
            faq_id=None,
            topic=None,
        )
        return {
            "reply": bot_reply,
            "source": "system",
            "faq_id": None,
            "topic": None,
        }

    # 1. Th·ª≠ tr·∫£ l·ªùi b·∫±ng FAQ tr∆∞·ªõc
    faq_result = find_faq_answer(user_text)
    if faq_result is not None:
        faq_answer = faq_result.get("answer", "")
        faq_id = faq_result.get("id")
        topic = faq_result.get("topic")
        log_chat(
            user_text=user_text,
            reply=faq_answer,
            source="faq",
            faq_id=faq_id,
            topic=topic,
        )
        return {
            "reply": faq_answer,
            "source": "faq",
            "faq_id": faq_id,
            "topic": topic,
        }

    # 2. Kh√¥ng c√≥ trong FAQ -> g·ªçi AI v·ªõi fallback (Gemini -> ChatGPT)
    ai_answer = ask_ai_with_fallback(user_text)
    if ai_answer is not None:
        log_chat(
            user_text=user_text,
            reply=ai_answer,
            source="ai",
            faq_id=None,
            topic=None,
        )
        return {
            "reply": ai_answer,
            "source": "ai",
            "faq_id": None,
            "topic": None,
        }

    # 3. C·∫£ FAQ v√† AI ƒë·ªÅu l·ªói -> tr·∫£ v·ªÅ th√¥ng b√°o h·ªá th·ªëng
    fallback_reply = (
        "Hi·ªán t·∫°i h·ªá th·ªëng AI ƒëang g·∫∑p s·ª± c·ªë n√™n m√¨nh ch∆∞a tr·∫£ l·ªùi chi ti·∫øt ƒë∆∞·ª£c. "
        "B·∫°n vui l√≤ng th·ª≠ l·∫°i sau ho·∫∑c li√™n h·ªá ph√≤ng ƒë√†o t·∫°o ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£."
    )
    log_chat(
        user_text=user_text,
        reply=fallback_reply,
        source="system",
        faq_id=None,
        topic=None,
    )
    return {
        "reply": fallback_reply,
        "source": "system",
        "faq_id": None,
        "topic": None,
    }




if __name__ == "__main__":
    uvicorn.run("main:app", host="127.0.0.1", port=8000, reload=True)
